{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b972c9ba",
   "metadata": {},
   "source": [
    "## Benchmark Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# XRESNET1D BENCHMARK REPLICATION - PURE PYTORCH\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import wfdb\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_ptbxl_dataset(path, sampling_rate=100):\n",
    "    \"\"\"Load PTB-XL dataset\"\"\"\n",
    "    # Load annotations\n",
    "    Y = pd.read_csv(path + 'ptbxl_database.csv', index_col='ecg_id')\n",
    "    Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "    # Load raw signals\n",
    "    X = load_raw_signals(Y, sampling_rate, path)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def load_raw_signals(df, sampling_rate, path):\n",
    "    \"\"\"Load raw ECG signals\"\"\"\n",
    "    cache_file = path + f'raw{sampling_rate}.npy'\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading cached data from {cache_file}\")\n",
    "        data = np.load(cache_file, allow_pickle=True)\n",
    "    else:\n",
    "        print(f\"Loading and caching raw signals at {sampling_rate}Hz\")\n",
    "        if sampling_rate == 100:\n",
    "            data = [wfdb.rdsamp(path + f) for f in tqdm(df.filename_lr)]\n",
    "        else:  # 500 Hz\n",
    "            data = [wfdb.rdsamp(path + f) for f in tqdm(df.filename_hr)]\n",
    "        \n",
    "        data = np.array([signal for signal, meta in data])\n",
    "        np.save(cache_file, data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: LABEL PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def aggregate_diagnostic_labels(df, scp_statements_path):\n",
    "    \"\"\"Aggregate SCP codes into superclasses\"\"\"\n",
    "    aggregation_df = pd.read_csv(scp_statements_path, index_col=0)\n",
    "    diag_agg_df = aggregation_df[aggregation_df.diagnostic == 1.0]\n",
    "    \n",
    "    def aggregate_diagnostic(y_dic):\n",
    "        tmp = []\n",
    "        for key in y_dic.keys():\n",
    "            if key in diag_agg_df.index:\n",
    "                c = diag_agg_df.loc[key].diagnostic_class\n",
    "                if str(c) != 'nan':\n",
    "                    tmp.append(c)\n",
    "        return list(set(tmp))\n",
    "    \n",
    "    df['diagnostic_superclass'] = df.scp_codes.apply(aggregate_diagnostic)\n",
    "    df['diagnostic_len'] = df.diagnostic_superclass.apply(lambda x: len(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_labels(X, Y, min_samples=0):\n",
    "    \"\"\"Convert to multi-hot encoding\"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    "    # Filter by minimum samples\n",
    "    counts = pd.Series(np.concatenate(Y.diagnostic_superclass.values)).value_counts()\n",
    "    counts = counts[counts > min_samples]\n",
    "    \n",
    "    Y.diagnostic_superclass = Y.diagnostic_superclass.apply(\n",
    "        lambda x: list(set(x).intersection(set(counts.index.values)))\n",
    "    )\n",
    "    Y['diagnostic_len'] = Y.diagnostic_superclass.apply(lambda x: len(x))\n",
    "    \n",
    "    # Remove samples with no labels\n",
    "    X = X[Y.diagnostic_len > 0]\n",
    "    Y = Y[Y.diagnostic_len > 0]\n",
    "    \n",
    "    # Transform to multi-hot\n",
    "    mlb.fit(Y.diagnostic_superclass.values)\n",
    "    y = mlb.transform(Y.diagnostic_superclass.values)\n",
    "    \n",
    "    print(f\"Classes: {mlb.classes_}\")\n",
    "    print(f\"Number of samples: {len(X)}\")\n",
    "    print(f\"Class distribution:\\n{pd.Series(np.concatenate(Y.diagnostic_superclass.values)).value_counts()}\")\n",
    "    \n",
    "    return X, Y, y, mlb\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_signals(X_train, X_val, X_test):\n",
    "    \"\"\"Standardize signals\"\"\"\n",
    "    # Fit on training data\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack(X_train).flatten()[:, np.newaxis].astype(float))\n",
    "    \n",
    "    # Apply to all sets\n",
    "    X_train_scaled = apply_standardizer(X_train, ss)\n",
    "    X_val_scaled = apply_standardizer(X_val, ss)\n",
    "    X_test_scaled = apply_standardizer(X_test, ss)\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, ss\n",
    "\n",
    "def apply_standardizer(X, ss):\n",
    "    \"\"\"Apply standardization to signals\"\"\"\n",
    "    X_tmp = []\n",
    "    for x in tqdm(X, desc=\"Standardizing\"):\n",
    "        x_shape = x.shape\n",
    "        X_tmp.append(ss.transform(x.flatten()[:, np.newaxis]).reshape(x_shape))\n",
    "    return np.array(X_tmp)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: XRESNET1D ARCHITECTURE (PURE PYTORCH)\n",
    "# ============================================================================\n",
    "\n",
    "class BasicBlock1d(nn.Module):\n",
    "    \"\"\"Basic ResNet block for 1D signals\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                               stride=stride, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size,\n",
    "                               stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class XResNet1d101(nn.Module):\n",
    "    \"\"\"XResNet1d-101 architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=12, num_classes=5, base_filters=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = base_filters\n",
    "        \n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, base_filters, kernel_size=7, stride=2, \n",
    "                     padding=3, bias=False),\n",
    "            nn.BatchNorm1d(base_filters),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # ResNet blocks [3, 4, 23, 3] for ResNet-101\n",
    "        self.layer1 = self._make_layer(base_filters, 3, stride=1)\n",
    "        self.layer2 = self._make_layer(base_filters*2, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(base_filters*4, 23, stride=2)\n",
    "        self.layer4 = self._make_layer(base_filters*8, 3, stride=2)\n",
    "        \n",
    "        # Head\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(base_filters*8*2, num_classes)  # *2 for concat pooling\n",
    "    \n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        \n",
    "        # First block may have stride > 1\n",
    "        layers.append(BasicBlock1d(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        # Remaining blocks\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(BasicBlock1d(self.in_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Concatenated pooling\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        \n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: PYTORCH DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for ECG signals\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X).permute(0, 2, 1)  # (N, time, channels) -> (N, channels, time)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            all_preds.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    return running_loss / len(dataloader.dataset), all_preds, all_labels\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: EVALUATION METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_scores):\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    # Macro AUC\n",
    "    macro_auc = roc_auc_score(y_true, y_scores, average='macro')\n",
    "    \n",
    "    # F-beta score (beta=2 as in benchmark)\n",
    "    f_beta = fbeta_score(y_true, y_pred, beta=2, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'macro_auc': macro_auc,\n",
    "        'f_beta_macro': f_beta\n",
    "    }\n",
    "\n",
    "def find_optimal_thresholds(y_true, y_scores):\n",
    "    \"\"\"Find optimal thresholds per class\"\"\"\n",
    "    from sklearn.metrics import roc_curve\n",
    "    \n",
    "    thresholds = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        fpr, tpr, threshold = roc_curve(y_true[:, i], y_scores[:, i])\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        thresholds.append(threshold[optimal_idx])\n",
    "    \n",
    "    return np.array(thresholds)\n",
    "\n",
    "def apply_thresholds(y_scores, thresholds):\n",
    "    \"\"\"Apply class-wise thresholds\"\"\"\n",
    "    y_pred = (y_scores > thresholds).astype(int)\n",
    "    \n",
    "    # If no prediction, take the maximum\n",
    "    for i, pred in enumerate(y_pred):\n",
    "        if pred.sum() == 0:\n",
    "            y_pred[i, np.argmax(y_scores[i])] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def plot_confusion_matrices(y_true, y_pred, class_names, save_path=None):\n",
    "    \"\"\"Plot confusion matrix for each class\"\"\"\n",
    "    n_classes = y_true.shape[1]\n",
    "    fig, axes = plt.subplots(1, n_classes, figsize=(4*n_classes, 4))\n",
    "    \n",
    "    if n_classes == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (ax, class_name) in enumerate(zip(axes, class_names)):\n",
    "        cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_title(f'{class_name}')\n",
    "        ax.set_ylabel('True')\n",
    "        ax.set_xlabel('Predicted')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: MAIN TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    DATA_PATH = '/path/to/ptbxl/'  # UPDATE THIS\n",
    "    SAMPLING_RATE = 100\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 50\n",
    "    LR = 0.001\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"XRESNET1D101 BENCHMARK REPLICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"\\n[1/8] Loading PTB-XL dataset...\")\n",
    "    X, Y = load_ptbxl_dataset(DATA_PATH, SAMPLING_RATE)\n",
    "    \n",
    "    # Step 2: Process labels\n",
    "    print(\"\\n[2/8] Processing labels...\")\n",
    "    Y = aggregate_diagnostic_labels(Y, DATA_PATH + 'scp_statements.csv')\n",
    "    X, Y, y, mlb = prepare_labels(X, Y, min_samples=0)\n",
    "    \n",
    "    # Step 3: Split data (folds 1-8: train, 9: val, 10: test)\n",
    "    print(\"\\n[3/8] Splitting data...\")\n",
    "    X_train = X[Y.strat_fold <= 8]\n",
    "    y_train = y[Y.strat_fold <= 8]\n",
    "    \n",
    "    X_val = X[Y.strat_fold == 9]\n",
    "    y_val = y[Y.strat_fold == 9]\n",
    "    \n",
    "    X_test = X[Y.strat_fold == 10]\n",
    "    y_test = y[Y.strat_fold == 10]\n",
    "    \n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Step 4: Preprocess\n",
    "    print(\"\\n[4/8] Preprocessing signals...\")\n",
    "    X_train, X_val, X_test, scaler = preprocess_signals(X_train, X_val, X_test)\n",
    "    \n",
    "    # Step 5: Create datasets\n",
    "    print(\"\\n[5/8] Creating PyTorch datasets...\")\n",
    "    train_dataset = ECGDataset(X_train, y_train)\n",
    "    val_dataset = ECGDataset(X_val, y_val)\n",
    "    test_dataset = ECGDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Step 6: Create model\n",
    "    print(\"\\n[6/8] Creating XResNet1d101 model...\")\n",
    "    model = XResNet1d101(input_channels=12, num_classes=len(mlb.classes_)).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "    \n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Step 7: Train\n",
    "    print(\"\\n[7/8] Training model...\")\n",
    "    best_val_auc = 0.0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_preds, val_labels = validate(model, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        # Compute metrics\n",
    "        thresholds = find_optimal_thresholds(val_labels, val_preds)\n",
    "        val_pred_binary = apply_thresholds(val_preds, thresholds)\n",
    "        val_metrics = compute_metrics(val_labels, val_pred_binary, val_preds)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val AUC: {val_metrics['macro_auc']:.4f} | Val F-beta: {val_metrics['f_beta_macro']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['macro_auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['macro_auc']\n",
    "            torch.save(model.state_dict(), 'best_xresnet1d101.pth')\n",
    "            print(f\"✓ Saved best model (AUC: {best_val_auc:.4f})\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    # Step 8: Test\n",
    "    print(\"\\n[8/8] Testing on test set...\")\n",
    "    model.load_state_dict(torch.load('best_xresnet1d101.pth'))\n",
    "    test_loss, test_preds, test_labels = validate(model, test_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Optimize thresholds on validation set\n",
    "    _, val_preds, val_labels = validate(model, val_loader, criterion, DEVICE)\n",
    "    thresholds = find_optimal_thresholds(val_labels, val_preds)\n",
    "    \n",
    "    # Apply to test set\n",
    "    test_pred_binary = apply_thresholds(test_preds, thresholds)\n",
    "    test_metrics = compute_metrics(test_labels, test_pred_binary, test_preds)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL TEST RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Test AUC (Macro): {test_metrics['macro_auc']:.4f}\")\n",
    "    print(f\"Test F-beta (Macro): {test_metrics['f_beta_macro']:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    plot_confusion_matrices(test_labels, test_pred_binary, mlb.classes_, \n",
    "                           save_path='confusion_matrices.png')\n",
    "    \n",
    "    print(\"\\n✓ Training complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
